{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63becd75",
   "metadata": {},
   "source": [
    "# Text Generation using GPT-2 Model\n",
    "\n",
    "This notebook demonstrates how to load and run a pre-trained GPT-2 model to generate text based on a user prompt. We'll use the `transformers` library from Hugging Face to load the model and handle the text generation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef83e462",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers torch ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dc4b0d",
   "metadata": {},
   "source": [
    "### Install the necessary libraries\n",
    "\n",
    "In this step, we install the `transformers` library, which provides the tools to load and use pre-trained models from Hugging Face, and `torch`, which is the PyTorch library used to run the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e373e8d2",
   "metadata": {},
   "source": [
    "### Import necessary libraries from Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a09de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fce2cd5",
   "metadata": {},
   "source": [
    "We are importing two important classes: `GPT2LMHeadModel` (for loading the GPT-2 model) and `GPT2Tokenizer` (for converting text to a format the model can understand). We also import `torch` to help manage the model and move it to GPU if available.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc023159",
   "metadata": {},
   "source": [
    "### Load the GPT-2 model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de7b9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2-medium\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2-medium\")\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab41e76",
   "metadata": {},
   "source": [
    "Here, we load the pre-trained `gpt2-medium` model and tokenizer. This step downloads the model from Hugging Face's model hub if you don't have it locally.\n",
    "This could take several minutes!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7a1bc4",
   "metadata": {},
   "source": [
    "### Function to generate text based on input prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2b42c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, num_return_sequences=1, temperature=0.85):\n",
    "    # Tokenize the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # Create an attention mask\n",
    "    attention_mask = torch.ones(input_ids.shape, device=device)\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(\n",
    "            input_ids,\n",
    "            max_length=max_length,\n",
    "            num_return_sequences=num_return_sequences,\n",
    "            no_repeat_ngram_size=2,\n",
    "            repetition_penalty=1.5,\n",
    "            top_p=0.92,\n",
    "            temperature=temperature,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            attention_mask=attention_mask,  # Provide attention mask\n",
    "            pad_token_id=tokenizer.eos_token_id  # Set pad token ID explicitly\n",
    "        )\n",
    "    \n",
    "    # Decode and return the generated text\n",
    "    generated_texts = [tokenizer.decode(output[i], skip_special_tokens=True) for i in range(num_return_sequences)]\n",
    "    return generated_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac6d89d",
   "metadata": {},
   "source": [
    "In this function:\n",
    "\n",
    "- We **tokenize** the input text (i.e., convert it into a format the model can process).\n",
    "- We use the model's `generate()` function to produce text based on the prompt. We set parameters like `max_length` (the maximum number of tokens to generate) and `temperature` (which controls the creativity/randomness of the generation).\n",
    "- The function returns the generated text, decoded back into human-readable format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c20640",
   "metadata": {},
   "source": [
    "### Generate text from a prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76682fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Text:\n",
      "\n",
      "1: Why did the chicken cross the road?\n",
      "In order to avoid being hit by a car, he would stop at red lights. He also started driving faster when approaching his destination after picking up chickens from farms and markets where there were already cars parked behind him. His behavior was out of control but as long it wasn't for what is supposed in this country...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = input(\"Enter a prompt: \")\n",
    "generated_texts = generate_text(prompt, max_length=100, num_return_sequences=1)\n",
    "\n",
    "print(\"\\nGenerated Text:\\n\")\n",
    "for idx, text in enumerate(generated_texts):\n",
    "    print(f\"{idx+1}: {text}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def6caae",
   "metadata": {},
   "source": [
    "This code block allows you to input a text prompt. The function will generate text based on that input and display the result(s).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e92314",
   "metadata": {},
   "source": [
    "### Interactive Widgets (Optional)\n",
    "\n",
    "To make the notebook more interactive, you can use widgets to adjust parameters like `max_length` and `temperature`. Below is an example of how you can add a slider for `max_length` and `temperature`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6478be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "prompt = widgets.Text(value='Enter your text here', description='Prompt:', disabled=False)\n",
    "max_length = widgets.IntSlider(min=50, max=300, step=10, value=100, description='Max Length')\n",
    "temperature = widgets.FloatSlider(min=0.1, max=1.5, step=0.1, value=0.85, description='Temperature')\n",
    "\n",
    "display(prompt, max_length, temperature)\n",
    "\n",
    "def on_button_clicked(b):\n",
    "    generated_texts = generate_text(prompt.value, max_length=max_length.value, temperature=temperature.value)\n",
    "    for idx, text in enumerate(generated_texts):\n",
    "        print(f\"{idx+1}: {text}\\n\")\n",
    "\n",
    "button = widgets.Button(description=\"Generate Text\")\n",
    "button.on_click(on_button_clicked)\n",
    "display(button)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
